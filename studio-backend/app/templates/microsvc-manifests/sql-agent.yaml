apiVersion: v1
kind: ConfigMap
metadata:
  name: config-{endpoint}
data:
  # ip_address: "${public_host_ip}"
  no_proxy: "${NO_PROXY}"
  http_proxy: "${HTTP_PROXY}"
  https_proxy: "${HTTP_PROXY}"
  llm_engine: "{llmEngine}"
  strategy: "{strategy}"
  db_name: "{db_name}"
  db_path: "{db_path}"
  recursion_limit: "{recursionLimit}"
  model: "{modelName}"
  temperature: "{temperature}"
  max_new_tokens: "{maxNewToken}"
  stream: "false"
  require_human_feedback: "false"
  llm_endpoint_url: "http://{llm_endpoint}:{llm_port}"
  PORT: "9096"
  OPENAI_API_KEY: "{openaiApiKey}"
---
apiVersion: v1
kind: Service
metadata:
  name: "{endpoint}"
spec:
  selector:
    app: "{endpoint}"
  ports:
    - protocol: TCP
      port: "{port}"
      targetPort: 9096
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: "{endpoint}"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "{endpoint}"
  template:
    metadata:
      labels:
        app: "{endpoint}"
    spec:
      containers:
        - name: sql-agent-container
          image: opea/agent:1.3
          command: ["/bin/sh", "-c"]
          args:
            - |
              export port=$(echo ${PORT} | awk '{print int($1)}')
              export temperature=$(printf "%.6f" "${temperature}")
              export recursion_limit=$(echo ${recursion_limit} | awk '{print int($1)}')
              export max_new_tokens=$(echo ${max_new_tokens} | awk '{print int($1)}')

              python agent.py
          ports:
            - containerPort: 9096
          # volumeMounts:
          #   - name: agent-tools
          #     mountPath: /home/user/tools/
          envFrom:
            - configMapRef:
                name: config-{endpoint}
      volumes:
        - name: agent-tools
          emptyDir: {}