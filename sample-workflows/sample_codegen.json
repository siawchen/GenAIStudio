{
  "nodes": [
    {
      "id": "chat_input_0",
      "position": {
        "x": 526.4999999999999,
        "y": 285.25
      },
      "type": "customNode",
      "data": {
        "label": "Chat Input",
        "name": "chat_input",
        "version": 1,
        "type": "ChatCompletionRequest",
        "icon": "/usr/src/packages/server/src/nodes/assets/controls.png",
        "category": "Controls",
        "description": "User Input from Chat Window",
        "baseClasses": [
          "ChatCompletionRequest"
        ],
        "tags": [
          "OPEA"
        ],
        "inMegaservice": false,
        "filePath": "/usr/src/packages/server/src/nodes/chat_input.js",
        "inputAnchors": [],
        "inputParams": [],
        "inputs": {},
        "outputs": {},
        "outputAnchors": [
          {
            "id": "chat_input_0-output-chat_input-ChatCompletionRequest",
            "name": "chat_input",
            "label": "ChatCompletionRequest",
            "description": "User Input from Chat Window",
            "type": "ChatCompletionRequest"
          }
        ],
        "id": "chat_input_0",
        "selected": false
      },
      "width": 300,
      "height": 143,
      "selected": false,
      "positionAbsolute": {
        "x": 526.4999999999999,
        "y": 285.25
      },
      "dragging": false
    },
    {
      "id": "opea_service@llm_codegen_0",
      "position": {
        "x": 888.5874400000002,
        "y": 204.56385999999998
      },
      "type": "customNode",
      "data": {
        "label": "LLM Code Generation",
        "name": "opea_service@llm_codegen",
        "version": 1,
        "type": "GeneratedDoc",
        "icon": "/usr/src/packages/server/src/nodes/assets/llm.png",
        "category": "LLM",
        "description": "LLM Code Generation Inference",
        "baseClasses": [
          "GeneratedDoc",
          "StreamingResponse"
        ],
        "tags": [
          "OPEA"
        ],
        "inMegaservice": true,
        "dependent_services": {
          "tgi": {
            "modelName": "",
            "huggingFaceToken": ""
          }
        },
        "inputs": {
          "text": "{{chat_input_0.data.instance}}",
          "modelName": "Qwen/Qwen2.5-Coder-7B-Instruct",
          "huggingFaceToken": "",
          "max_tokens": 17,
          "top_k": 10,
          "top_p": 0.95,
          "typical_p": 0.95,
          "temperature": 0.01,
          "presence_penalty": 1.03,
          "frequency_penalty": "",
          "streaming": true,
          "chat_template": "### You are a helpful, respectful and honest assistant to help the user with questions.\n### Context: {context}\n### Question: {question}\n### Answer:"
        },
        "filePath": "/usr/src/packages/server/src/nodes/llm_codegen.js",
        "inputAnchors": [
          {
            "label": "LLM Params Document",
            "name": "text",
            "type": "LLMParamsDoc|ChatCompletionRequest",
            "id": "opea_service@llm_codegen_0-input-text-LLMParamsDoc|ChatCompletionRequest"
          }
        ],
        "inputParams": [
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "default": "Qwen/Qwen2.5-Coder-7B-Instruct",
            "id": "opea_service@llm_codegen_0-input-modelName-string"
          },
          {
            "label": "HuggingFace Token",
            "name": "huggingFaceToken",
            "type": "password",
            "optional": true,
            "id": "opea_service@llm_codegen_0-input-huggingFaceToken-password"
          },
          {
            "label": "Maximum Tokens",
            "name": "max_tokens",
            "type": "number",
            "default": 17,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-max_tokens-number"
          },
          {
            "label": "Top K",
            "name": "top_k",
            "type": "number",
            "default": 10,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-top_k-number"
          },
          {
            "label": "Top P",
            "name": "top_p",
            "type": "number",
            "default": 0.95,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-top_p-number"
          },
          {
            "label": "Typical P",
            "name": "typical_p",
            "type": "number",
            "default": 0.95,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-typical_p-number"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "default": 0.01,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-temperature-number"
          },
          {
            "label": "Presence Penalty",
            "name": "presence_penalty",
            "type": "number",
            "default": 1.03,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-presence_penalty-number"
          },
          {
            "label": "Frequency Penalty",
            "name": "frequency_penalty",
            "type": "number",
            "default": 0,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-frequency_penalty-number"
          },
          {
            "label": "Streaming",
            "name": "streaming",
            "type": "boolean",
            "default": true,
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-streaming-boolean"
          },
          {
            "label": "Chat Template",
            "name": "chat_template",
            "type": "string",
            "rows": true,
            "default": "### You are a helpful, respectful and honest assistant to help the user with questions.\n### Context: {context}\n### Question: {question}\n### Answer:",
            "optional": true,
            "additionalParams": true,
            "id": "opea_service@llm_codegen_0-input-chat_template-string"
          }
        ],
        "outputs": {},
        "outputAnchors": [
          {
            "id": "opea_service@llm_codegen_0-output-opea_service@llm_codegen-GeneratedDoc|StreamingResponse",
            "name": "opea_service@llm_codegen",
            "label": "GeneratedDoc",
            "description": "LLM Code Generation Inference",
            "type": "GeneratedDoc | StreamingResponse"
          }
        ],
        "id": "opea_service@llm_codegen_0",
        "selected": false
      },
      "width": 300,
      "height": 481,
      "selected": false,
      "positionAbsolute": {
        "x": 888.5874400000002,
        "y": 204.56385999999998
      },
      "dragging": false
    },
    {
      "id": "chat_completion_1",
      "position": {
        "x": 1294.0750736940474,
        "y": 314.4780045886494
      },
      "type": "customNode",
      "data": {
        "label": "Chat Completion",
        "name": "chat_completion",
        "version": 1,
        "type": "ChatCompletion",
        "icon": "/usr/src/packages/server/src/nodes/assets/controls.png",
        "category": "Controls",
        "description": "Send Chat Response to UI",
        "baseClasses": [],
        "tags": [
          "OPEA"
        ],
        "inMegaservice": false,
        "inputs": {
          "llm_response": "{{opea_service@llm_codegen_0.data.instance}}",
          "ui_choice": "code"
        },
        "hideOutput": true,
        "filePath": "/usr/src/packages/server/src/nodes/chat_completion.js",
        "inputAnchors": [
          {
            "label": "LLM Response",
            "name": "llm_response",
            "type": "StreamingResponse|ChatCompletion",
            "id": "chat_completion_1-input-llm_response-StreamingResponse|ChatCompletion"
          }
        ],
        "inputParams": [
          {
            "label": "UI Choice",
            "name": "ui_choice",
            "type": "options",
            "default": "chat",
            "options": [
              {
                "name": "chat",
                "label": "Chat Q&A"
              },
              {
                "name": "summary",
                "label": "Summarize Content"
              },
              {
                "name": "code",
                "label": "Generate Code"
              }
            ],
            "id": "chat_completion_1-input-ui_choice-options"
          }
        ],
        "outputs": {},
        "outputAnchors": [],
        "id": "chat_completion_1",
        "selected": false
      },
      "width": 300,
      "height": 239,
      "positionAbsolute": {
        "x": 1294.0750736940474,
        "y": 314.4780045886494
      },
      "selected": false,
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "chat_input_0",
      "sourceHandle": "chat_input_0-output-chat_input-ChatCompletionRequest",
      "target": "opea_service@llm_codegen_0",
      "targetHandle": "opea_service@llm_codegen_0-input-text-LLMParamsDoc|ChatCompletionRequest",
      "type": "buttonedge",
      "id": "chat_input_0-chat_input_0-output-chat_input-ChatCompletionRequest-opea_service@llm_codegen_0-opea_service@llm_codegen_0-input-text-LLMParamsDoc|ChatCompletionRequest"
    },
    {
      "source": "opea_service@llm_codegen_0",
      "sourceHandle": "opea_service@llm_codegen_0-output-opea_service@llm_codegen-GeneratedDoc|StreamingResponse",
      "target": "chat_completion_1",
      "targetHandle": "chat_completion_1-input-llm_response-StreamingResponse|ChatCompletion",
      "type": "buttonedge",
      "id": "opea_service@llm_codegen_0-opea_service@llm_codegen_0-output-opea_service@llm_codegen-GeneratedDoc|StreamingResponse-chat_completion_1-chat_completion_1-input-llm_response-StreamingResponse|ChatCompletion"
    }
  ]
}